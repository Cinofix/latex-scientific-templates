\chapter{Introduction}
Nowadays we have enough knowledge about the ability of machine learning models to make good predictions in different domains, like image classification, speech recognition, market analysis and image segmentation. Due to their incredible results, learning systems are assuming a fundamental role in very sophisticated applications as tools for aiding in decision making.
However, it has been observed that, despite their advanced capabilities, they are sensitive to adversarial perturbations in the input data, leading  algorithms to make wrong predictions. A very significant observation is that sometimes these alterations are invisible to human eyes, leaving us some doubts about how  these models manage data. The key problem is that these models have not been designed for working in scenarios where an attacker wants to subvert or compromise the results of the system.
Essentially an attacker can carefully craft adversarial samples to inject inside data for subverting the normal behavior of the machine learning model. In \cite{spamfilter,spamfilter2} the authors discussed the problems of adversarial perturbations for spam filtering systems. They show how linear classifiers can be easily fooled by simply injecting carefully crafted perturbations inside spam messages. Even other applications of machine learning models (like fraud detection, face recognition,  video surveillance, traffic prediction, credit-risk assessment, etc.) could be subject to malicious activities. Due to the sensitivity of these domains, defense strategies and robustness analyses have been studied for limiting the vulnerability to adversarial examples \cite{obfuscategradient,wildpatterns,adversarialexamples,distanceDefence,certifiedDefenses}. Nevertheless, security can be seen as an arms race game between attacker and designer. The former wants to subvert the system, on the other hand, the latter makes the system available and wants to protect it developing opportune countermeasures. During the game each player evolves its strategy in contrast to the other. Indeed, the designer develops defensive countermeasures against threats and the attacker develops new attacks for breaking defensive strategies.
The key problem of machine learning models is that they are completely data-driven and data contains noise by nature. Consider the scenario of a network of temperature sensors in a room and, according to the retrieved temperature, the system reacts with appropriate strategies. Data collected by sensors are commonly subject to noise, due to internal and/or external variables. However, sensors can also be manipulated by an attacker for obtaining certain results. From this observation, we can see how it is difficult to detect when data are subject to natural noise or when they are affected by malicious threats, named adversarial noise.\\
In the rest of this Chapter we are going to introduce the theory behind the adversarial machine learning field, limits of the current state of the art and how this thesis is organized.
\newpage


\section{Adversarial Machine Learning Theory}
The sensitivity of machine learning models to adversarial noise has recently attracted a large interest in the computer science community. Different authors, such us \citeauthor{wildpatterns}, proposed a theoretical framework for studying the roles of attacker and designer in this arms race game. In their comprehensive review \cite{wildpatterns} they propose a framework for studying the security property of supervised models. In next works, \citeauthor{isdataclustering} \cite{isdataclustering} extended the initial framework for the unsupervised paradigm. In particular, they provided a taxonomy about possible adversary's goals and knowledge.


\subsection{Adversary's Goal}
The adversary's goal defines the objective function that the attacker wants to maximize or minimize by injecting adversarial examples to the system. Essentially the attacker may be interested on affecting three security properties: integrity, availability, confidentiality. 

\paragraph{Integrity Violation}
The attacker performs malicious activities that do not compromise significantly the system behavior. In the clustering scenario we refer to the violation of the resulting grouping. Indeed, with integrity violation the attackers aims to subvert the grouping for a portion of samples preserving the original grouping as much as possible for the rest of the sample. For example, given a dataset $X$ containing two groups of users: \texttt{authorized} or \texttt{guest}. The attacker may inject well crafted adversarial perturbations for moving samples from \texttt{guest} towards \texttt{authorized}.


\paragraph{Availability Violation} 
The attacker performs malicious activities for limiting functionalities or access to the system. In the clustering scenario the attackers wants to subvert completely the clustering process. Considering the \texttt{authorized} and \texttt{guest} groups, the attacker injects noise such that at the end the two clusters are reversed or they are merged together.

\paragraph{Confidentiality Violation} The attacker performs malicious activities in order to extract private information about the composition of the clusters. Considering the \texttt{authorized} and \texttt{guest} clusters, the attacker may extract fundamental features from \texttt{authorized} users by applying strategies of reverse-engineer of the clustering process.\\


For all the 3 cases the attacker may also define a certain specificity, that could be targeted or indiscriminate. In the first case a targeted subset of samples is subject to adversarial perturbations. On the other hand, any sample can be manipulated without any distinction with respect to the others. For example, consider a dataset composed by three clusters: \texttt{authorized}, \texttt{guest} and \texttt{rejected}. If the attacker moves explicitly samples from rejected towards authorized, then we define the attacker's goal as targeted. Conversely, if the attacker moves samples without a target direction, then we define the attacker's goal as indiscriminate. In the proposed example, this would mean that moving samples from \texttt{guest} towards \texttt{authorized} gives the same payoff as moving samples from \texttt{guest} towards \texttt{rejected}.


\subsection{Adversary's Knowledge}
The adversary's knowledge defines the capacity of the attacker to inject malicious threats. The more knowledge the attacker has about the system, the greater the attacker's capacity is. It is reasonable to think that if the attacker knows perfectly the system, then it is easier is for him/her to craft appropriate threats. Conversely, if the attacker has no knowledge about the system, it is more difficult to craft threats against a black box system.\\
\citeauthor{isdataclustering} provided in \cite{isdataclustering} a taxonomy of attacker's knowledge:
\begin{itemize}
	\item Knowledge of data $\mathcal{D}$: the attacker knows exactly the composition of the data samples taken into consideration. It could be unnecessary the complete knowledge but a portion of the entire collection might suffice. $\mathcal{D}$ is drawn from an unknown probability distribution $p$.
	\item Knowledge of the feature space: the attacker knows exactly the features composition of data samples or how they are obtained. 
	\item Knowledge of the algorithm: the attacker knows which algorithm is used for clustering data samples and how similarity between them is computed if necessary.
	\item Knowledge of the algorithm's parameters: the attacker knows the parameters provided to the clustering algorithm (ex: the number of clusters $k$).
\end{itemize}
The best scenario for the attacker, and consequently the worst case for the designer, is the one in which he/she has full knowledge (Perfect Knowledge) of the target system. Certainly, this is not always the case, bringing the attacker to a Limited knowledge, or worst, in a Zero knowledge. Even if the attacker has zero knowledge some strategies can be adopted for estimating the required components. For example, in absence of knowledge about $\mathcal{D}$ the attacker can collect a surrogate dataset $\hat{\mathcal{D}}$ with the hope that samples are retrieved from the same distribution $p$. For instance, if the target system uses clustering of malware samples, then the attacker can collect a surrogate dataset of malware $\hat{\mathcal{D}}$ and use them as an estimate of the true dataset $\mathcal{D}$.
The attacker may also have no knowledge about the features representation used for projecting samples in the space. In certain conditions it could be easier to identify the right feature representation since machine learning is getting more and more standardized to provide unified services. For example, documents are likely represented in vectors of TF-IDF, or images are likely defined in matrices of intensities or RGB values. \\
A common argument in cyber-security is that in order to build a secure system, it should overestimate the attacker's capabilities rather than underestimating them.

\section{Problem Description}
Because of its strong implications, adversarial machine learning theory has been developed more and more in the latest years' research works. The greatest part of the research, including \cite{evasionattacks,adversarialexamples,  PapernoTransferability,rademacher}, addresses the problem of adversarial examples against supervised learning. In these works authors try to give explanations to the adversarial phenomenon going deeper on how models work. Especially, \citeauthor{rademacher} give a first connection between adversarial learning and statistical learning theory, with the usage of the Rademacher complexity. \citeauthor{PapernoTransferability} introduce a key property of adversarial examples, which is their transferability between multiple models. Even defensive and adversary algorithms have been designed for crafting adversarial examples or for preventing the system against threats.\\
Conversely, from the best of our knowledge, only few works have been published against unsupervised learning paradigm applications. \citeauthor{isdataclustering} develop some strategies for fooling single-linkage hierarchical clustering, and later on, a similar work \cite{poisonclustering} has been developed against complete-linkage hierarchical clustering.
Despite this lack, the demanding techniques for cracking clustering models or for protecting them have recently turned to be fundamental. Clustering is finding a wide range of applications, due to the absence of labeled data, in very sensitive domains like image segmentation, face clustering, market or social analysis, information processing, etc.\\
The aim of this research is to investigate the robustness provided by some clustering algorithms in presence of very well-crafted adversarial examples. We provide three ways for crafting adversarial threats against clustering algorithms and we analyze how K-Means, Spectral and Dominant Sets clustering react against them.


\section{Outline}
For the remainder of this thesis, we organize our work as follows. In Chapter.2, we briefly introduce the background related to the unsupervised learning paradigm. More specifically we give a deeper introduction to the three clustering algorithm analyzed in the rest of this thesis (K-Means, Spectral and Dominant Sets clustering). We give knowledge about how these algorithms work, their formulation and properties. Following it in Chapter.3,  we introduce the 3 designed algorithms against clustering. In the first part we introduce the concept and applications of image segmentation and how sensitive could be that field in presence of adversarial noise. We discuss the two designed algorithms that can be used by an attacker for fooling image classification. Conversely, in the second part of Chapter.3, we introduce the sensitivity of certain clustering applications in possible adversarial settings. We provide and explain how the three designed adversarial algorithms work in order to fool data clustering algorithms.\\
In Chapter.4 we show the experiments done during the development of this thesis and we analyze the robustness provided by the three algorithms. We show, using different visualization techniques, how the three algorithms react by changing the attacker's capacity.\\
Finally, in Chapter.5, we conclude this thesis with discussions, open issues and we highlight better our contribution thanks to this thesis. At last, we propose a list of open issues, possible future improvements and ideas, realized during the development of this work, in order to give future research directions on adversarial machine learning.

